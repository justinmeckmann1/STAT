{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "754a795a",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f012f0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.gofplots import ProbPlot\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def fit_linear_reg(x, y):\n",
    "    '''Fit Linear model with predictors x on y \n",
    "    return AIC, BIC, R2 and R2 adjusted '''\n",
    "    x = sm.add_constant(x)\n",
    "    # Create and fit model\n",
    "    model_k = sm.OLS(y, x).fit()\n",
    "    \n",
    "    # Find scores\n",
    "    BIC = model_k.bic\n",
    "    AIC = model_k.aic\n",
    "    R2 = model_k.rsquared\n",
    "    R2_adj = model_k.rsquared_adj\n",
    "    RSS = model_k.ssr\n",
    "    \n",
    "    # Return result in Series\n",
    "    results = pd.Series(data={'BIC': BIC, 'AIC': AIC, 'R2': R2,\n",
    "                              'R2_adj': R2_adj, 'RSS': RSS})\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def add_one(x_full, x, y, scoreby='RSS'):\n",
    "    ''' Add possible predictors from x_full to x, \n",
    "    Fit a linear model on y using fit_linear_reg\n",
    "    Returns Dataframe showing scores as well as best model '''\n",
    "    # Predefine DataFrame\n",
    "    x_labels = x_full.columns\n",
    "    zeros = np.zeros(len(x_labels))\n",
    "    results = pd.DataFrame(\n",
    "        data={'Predictor': x_labels.values, 'BIC': zeros, \n",
    "               'AIC': zeros, 'R2': zeros, \n",
    "               'R2_adj': zeros, 'RSS': zeros})\n",
    "\n",
    "    # For every predictor find R^2, RSS, and AIC\n",
    "    for i in range(len(x_labels)):\n",
    "        x_i = np.concatenate((x, [np.array(x_full[x_labels[i]])]))\n",
    "        results.iloc[i, 1:] = fit_linear_reg(x_i.T, y)\n",
    "        \n",
    "    # Depending on where we scoreby, we select the highest or lowest\n",
    "    if scoreby in ['RSS', 'AIC', 'BIC']:\n",
    "        best = x_labels[results[scoreby].argmin()]\n",
    "    elif scoreby in ['R2', 'R2_adj']:\n",
    "        best = x_labels[results[scoreby].argmax()]\n",
    "        \n",
    "    return results, best \n",
    "\n",
    "def drop_one(x, y, scoreby='RSS'):\n",
    "    ''' Remove possible predictors from x, \n",
    "    Fit a linear model on y using fit_linear_reg\n",
    "    Returns Dataframe showing scores as well as predictor \n",
    "    to drop in order to keep the best model '''\n",
    "    # Predefine DataFrame\n",
    "    x_labels = x.columns\n",
    "    zeros = np.zeros(len(x_labels))\n",
    "    results = pd.DataFrame(\n",
    "        data={'Predictor': x_labels.values, 'BIC': zeros, \n",
    "               'AIC': zeros, 'R2': zeros, \n",
    "               'R2_adj': zeros, 'RSS': zeros})\n",
    "\n",
    "    # For every predictor find RSS and R^2\n",
    "    for i in range(len(x_labels)):\n",
    "        x_i = x.drop(columns=x_labels[i])\n",
    "        results.iloc[i, 1:] = fit_linear_reg(x_i, y)\n",
    "    \n",
    "    # Depending on where we scoreby, we select the highest or lowest\n",
    "    if scoreby in ['RSS', 'AIC', 'BIC']:\n",
    "        worst = x_labels[results[scoreby].argmin()]\n",
    "    elif scoreby in ['R2', 'R2_adj']:\n",
    "        worst = x_labels[results[scoreby].argmax()]\n",
    "    \n",
    "    return results, worst \n",
    "\n",
    "\n",
    "\"\"\" Plot Residuals vs Fitted Values \"\"\"\n",
    "def plot_residuals(axes, res, yfit, n_samp=0):\n",
    "    \"\"\" Inputs:\n",
    "    axes: axes created with matplotlib.pyplot\n",
    "    x: x values\n",
    "    ytrue: y values\n",
    "    yfit: fitted/predicted y values\n",
    "    res[optional]: Residuals, used for resampling\n",
    "    n_samp[optional]: number of resamples \"\"\"\n",
    "    # For every random resampling\n",
    "    for i in range(n_samp):\n",
    "        # 1. resample indices from Residuals\n",
    "        samp_res_id = random.sample(list(res), len(res))\n",
    "        # 2. Average of Residuals, smoothed using LOWESS\n",
    "        sns.regplot(x=yfit, y=samp_res_id,\n",
    "        scatter=False, ci=False, lowess=True,\n",
    "        line_kws={'color': 'lightgrey', 'lw': 1, 'alpha': 0.8})\n",
    "        # 3. Repeat again for n_samples\n",
    "\n",
    "    dataframe = pd.concat([yfit, res], axis=1)\n",
    "    axes = sns.residplot(x=yfit, y=res, data=dataframe, \n",
    "                         lowess=True, scatter_kws={'alpha': 0.5}, \n",
    "                         line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n",
    "    axes.set_title('Residuals vs Fitted')\n",
    "    axes.set_ylabel('Residuals')\n",
    "    axes.set_xlabel('Fitted Values')\n",
    "    \n",
    "\"\"\" QQ Plot standardized residuals \"\"\"\n",
    "def plot_QQ(axes, res_standard, n_samp=0):\n",
    "    \"\"\" Inputs:\n",
    "    axes: axes created with matplotlib.pyplot\n",
    "    res_standard: standardized residuals\n",
    "    n_samp[optional]: number of resamples \"\"\"\n",
    "    \n",
    "    # QQ plot instance\n",
    "    QQ = ProbPlot(res_standard)\n",
    "    # Split the QQ instance in the seperate data\n",
    "    qqx = pd.Series(sorted(QQ.theoretical_quantiles), name=\"x\")\n",
    "    qqy = pd.Series(QQ.sorted_data, name=\"y\")\n",
    "    if n_samp != 0:\n",
    "        # Estimate the mean and standard deviation\n",
    "        mu = np.mean(qqy)\n",
    "        sigma = np.std(qqy)\n",
    "        # For ever random resampling\n",
    "        for lp in range(n_samp):\n",
    "            # Resample indices\n",
    "            samp_res_id = np.random.normal(mu, sigma, len(qqx))\n",
    "            # Plot\n",
    "            sns.regplot(x=qqx, y=sorted(samp_res_id),\n",
    "            scatter=False, ci=False, lowess=True,\n",
    "            line_kws={'color': 'lightgrey', 'lw': 1, 'alpha': 0.8})\n",
    "\n",
    "            sns.regplot(x=qqx, y=qqy, scatter=True, lowess=False, \n",
    "                        ci=False, scatter_kws={'s': 40, 'alpha': 0.5}, \n",
    "                        line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n",
    "            \n",
    "    axes.plot(qqx, qqx, '--k', alpha=0.5)\n",
    "    axes.set_title('Normal Q-Q')\n",
    "    axes.set_xlabel('Theoretical Quantiles')\n",
    "    axes.set_ylabel('Standardized Residuals')\n",
    "\n",
    "\"\"\" Scale-Location Plot \"\"\"\n",
    "def plot_scale_loc(axes, yfit, res_stand_sqrt, n_samp=0):\n",
    "    \"\"\" Inputs:\n",
    "    axes: axes created with matplotlib.pyplot\n",
    "    yfit: fitted/predicted y values\n",
    "    res_stand_sqrt: Absolute square root Residuals\n",
    "    n_samp[optional]: number of resamples \"\"\"\n",
    "    \n",
    "    # For every random resampling\n",
    "    for i in range(n_samp):\n",
    "    # 1. resample indices from sqrt Residuals\n",
    "        samp_res_id = random.sample(list(res_stand_sqrt), len(res_stand_sqrt))\n",
    "        # 2. Average of Residuals, smoothed using LOWESS\n",
    "        sns.regplot(x=yfit, y=samp_res_id,\n",
    "                    scatter=False, ci=False, lowess=True,\n",
    "                    line_kws={'color': 'lightgrey', 'lw': 1, 'alpha': 0.8})\n",
    "        # 3. Repeat again for n_samples\n",
    "\n",
    "    # plot Regression usung Seaborn\n",
    "    sns.regplot(x=yfit, y=res_stand_sqrt,\n",
    "                scatter=True, ci=False, lowess=True,\n",
    "                scatter_kws={'s': 40, 'alpha': 0.5},\n",
    "                \n",
    "    line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n",
    "    axes.set_title('Scale-Location plot')\n",
    "    axes.set_xlabel('Fitted values')\n",
    "    axes.set_ylabel('$\\sqrt{\\|Standardized\\ Residuals\\|}$')\n",
    "\n",
    "def plot_cooks(axes, res_inf_leverage, res_standard, n_pred=1,\n",
    "               x_lim=None, y_lim=None, n_levels=4):\n",
    "    \"\"\" Inputs:\n",
    "    axes: axes created with matplotlib.pyplot\n",
    "    res_inf_leverage: Leverage\n",
    "    res_standard: standardized residuals\n",
    "    n_pred: number of predictor variables in x\n",
    "    x_lim, y_lim[optional]: axis limits\n",
    "    n_levels: number of levels\"\"\"\n",
    "    \n",
    "    sns.regplot(x=res_inf_leverage, y=res_standard,\n",
    "                scatter=True, ci=False, lowess=True,\n",
    "                scatter_kws={'s': 40, 'alpha': 0.5},\n",
    "                \n",
    "    line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n",
    "    # Set limits\n",
    "    if x_lim != None:\n",
    "        x_min, x_max = x_lim[0], x_lim[1]\n",
    "    else:\n",
    "        x_min, x_max = min(res_inf_leverage)*0.95, max(res_inf_leverage)*1.05\n",
    "    if y_lim != None:\n",
    "        y_min, y_max = y_lim[0], y_lim[1]\n",
    "    else:\n",
    "        y_min, y_max = min(res_standard)*0.95, max(res_standard)*1.05\n",
    "\n",
    "    # Plot centre line\n",
    "    plt.plot((x_min, x_max), (0, 0), 'g--', alpha=0.8)\n",
    "    # Plot contour lines for Cook's Distance levels\n",
    "    n = 100\n",
    "    cooks_distance = np.zeros((n, n))\n",
    "    x_cooks = np.linspace(x_min, x_max, n)\n",
    "    y_cooks = np.linspace(y_min, y_max, n)\n",
    "\n",
    "    for xi in range(n):\n",
    "        for yi in range(n):\n",
    "            cooks_distance[yi][xi] = \\\n",
    "            y_cooks[yi]**2 * x_cooks[xi] / (1 - x_cooks[xi]) / (n_pred + 1)\n",
    "            \n",
    "    CS = axes.contour(x_cooks, y_cooks, cooks_distance, levels=n_levels, alpha=0.6)\n",
    "\n",
    "    axes.clabel(CS, inline=0, fontsize=10)\n",
    "    axes.set_xlim(x_min, x_max)\n",
    "    axes.set_ylim(y_min, y_max)\n",
    "    axes.set_title('Residuals vs Leverage and Cook\\'s distance')\n",
    "    axes.set_xlabel('Leverage')\n",
    "    axes.set_ylabel('Standardized Residuals')\n",
    "    \n",
    "\"\"\" Standard scatter plot and regression line \"\"\"\n",
    "def plot_reg(axes, x, y, x_lab=\"x\", y_lab=\"y\", title=\"Linear Regression\"):\n",
    "    \"\"\" Inputs:\n",
    "    axes: axes created with matplotlib.pyplot\n",
    "    x: (single) Feature\n",
    "    y: Result \"\"\"\n",
    "    # Plot scatter data\n",
    "    sns.regplot(x=x, y=y,\n",
    "                scatter=True, ci=False, lowess=False,\n",
    "                scatter_kws={'s': 40, 'alpha': 0.5},\n",
    "                \n",
    "    line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n",
    "    # Set labels:\n",
    "    axes.set_xlabel(x_lab)\n",
    "    axes.set_ylabel(y_lab)\n",
    "    axes.set_title(title)\n",
    "\n",
    "\"\"\" VIF Analysis \"\"\"\n",
    "def VIF_analysis(x):\n",
    "    \"\"\" VIF analysis of variables saved in x\n",
    "    Input:\n",
    "    x: m*n matrix or Dataframe, containing m samples of n predictors\n",
    "    Output:\n",
    "    VIF: Vector containing n Variance Inflation Factors\n",
    "    \"\"\"\n",
    "    # Preproces:\n",
    "    x_np = x.to_numpy()\n",
    "    VIF = []\n",
    "    # For all n Predictors:\n",
    "    for i in range(x.shape[1]):\n",
    "        # Define x and y\n",
    "        x_i = np.delete(x_np, i, 1)\n",
    "        x_i = sm.add_constant(x_i)\n",
    "        y_i = x_np[:, i]\n",
    "        # Fit model\n",
    "        model = sm.OLS(y_i, x_i).fit()\n",
    "        # Calculate the VIF\n",
    "        VIF.append(1 / (1 - model.rsquared))\n",
    "    \n",
    "    return VIF\n",
    "\n",
    "def plot_resid_analysis(model):\n",
    "    # Find the predicted values for the original design.\n",
    "    yfit = model.fittedvalues\n",
    "    # Find the Residuals\n",
    "    res = model.resid\n",
    "    # Influence of the Residuals\n",
    "    res_inf = model.get_influence()\n",
    "    # Studentized residuals using variance from OLS\n",
    "    res_standard = res_inf.resid_studentized_internal\n",
    "    # Absolute square root Residuals:\n",
    "    res_stand_sqrt = np.sqrt(np.abs(res_standard))\n",
    "    # Cook's Distance and leverage:\n",
    "    res_inf_cooks = res_inf.cooks_distance\n",
    "    res_inf_leverage = res_inf.hat_matrix_diag\n",
    "\n",
    "    \"\"\" Plots \"\"\"\n",
    "    # Create Figure and subplots\n",
    "    fig = plt.figure(figsize = (12,9))\n",
    "\n",
    "    # First subplot: Residuals vs Fitted values with 100 resamples\n",
    "    ax1 = fig.add_subplot(2, 2, 1)\n",
    "    plot_residuals(ax1, res, yfit, n_samp = 100)\n",
    "\n",
    "    # Second subplot: QQ Plot with 100 resamples\n",
    "    ax2 = fig.add_subplot(2, 2, 2)\n",
    "    plot_QQ(ax2, res_standard, n_samp = 100)\n",
    "\n",
    "    # Third subplot: Scale-location with 100 resamples\n",
    "    ax3 = fig.add_subplot(2, 2, 3)\n",
    "    plot_scale_loc(ax3, yfit, res_stand_sqrt, n_samp = 100)\n",
    "\n",
    "    # Fourth subplot: Residuals vs Fitted values with 100 resamples\n",
    "    ax4 = fig.add_subplot(2, 2, 4)\n",
    "    plot_cooks(ax4, res_inf_leverage, res_standard, n_pred = model.df_model)\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fe91f9",
   "metadata": {},
   "source": [
    "#### Daten laden und Modell fitten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdeef67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('./data/fitness.csv')\n",
    "\n",
    "# Prepare Data\n",
    "x = df[['age','weight','runtime','rstpulse','runpulse','maxpulse']] # Predictor Variables\n",
    "x_sm = sm.add_constant(x)       # Add constant \n",
    "y = df['oxy']                   # Response Variable\n",
    "\n",
    "model = sm.OLS(y, x_sm).fit()   # Fit the model\n",
    "print(model.summary())          # Print summary "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daf8590",
   "metadata": {},
   "source": [
    "#### Residuenanalyse (zuerst helper script laden!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfb7c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_resid_analysis(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d327b4a",
   "metadata": {},
   "source": [
    "#### Korrelationsmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbe118b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response Variable 'oxy' wird entfernt\n",
    "print(round(df.drop(['oxy'],axis=1).corr(), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfedec8",
   "metadata": {},
   "source": [
    "#### VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c51685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1 = GUT, ab 5-10 hinweis für Multikolinearität)\n",
    "VIF = VIF_analysis(x)\n",
    "\n",
    "res = pd.DataFrame(data={'Predictor': x.columns,\n",
    "                         'VIF': np.round(VIF, 3)})\n",
    "print(res.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a548f5",
   "metadata": {},
   "source": [
    "#### VIF FIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3d7e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new variable durch kombinationen von Variablen mit hohem VIF\n",
    "x_ii = df.drop(['oxy', 'maxpulse'], axis=1) # Zielevariable 'oxy' und 'maxpulse' (hohes VIF) entfernen\n",
    "newvar = df['runpulse'] / df['maxpulse'] # neue Variable erstellen \n",
    "newvar = newvar.rename('run/max')\n",
    "x_ii = x_ii.join(newvar) # neue Variable hinzufügen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0d18a8",
   "metadata": {},
   "source": [
    "#### Paarweise Vergleich (zweier Modelle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23c910b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_i = model_i.predict(sm.add_constant(x_i))\n",
    "pred_ii = model_ii.predict(sm.add_constant(x_ii))\n",
    "\n",
    "fig = plt.figure(figsize = (6,5))\n",
    "ax1 = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "plot_reg(ax1, pred_i, pred_ii,\n",
    "         x_lab=\"pred_i\", y_lab=\"pred_ii\",\n",
    "         title=\"Comparing predictions\")\n",
    "plt.show()\n",
    "\n",
    "# Liegen die Punkte auf der Diagonalen, sind die Vorhersagen \n",
    "# identisch, falls nicht ist durch die exlusion von variablen \n",
    "# precision verloren gegangen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a818b18",
   "metadata": {},
   "source": [
    "#### Forward Stepwise Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3640f5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(data={'Best_Pred': [], 'AIC':[]})\n",
    "\n",
    "# Define the empty predictor\n",
    "x0 = [np.zeros(len(y))]\n",
    "\n",
    "x_red = x.copy() # Copy the full set of predictors\n",
    "x_forward = x0\n",
    "\n",
    "\n",
    "for i in range(x_red.shape[1]):\n",
    "    results_i, best_i = add_one(x_red, x_forward, y, scoreby='AIC')\n",
    "\n",
    "    # Update the empty predictor with the best predictor\n",
    "    x_forward = np.concatenate((x_forward, [x[best_i]]))\n",
    "\n",
    "    # Remove the chosen predictor from the list of options\n",
    "    x_red = x_red.drop(columns=best_i)\n",
    "\n",
    "    # Save results\n",
    "    results.loc[i, 'Best_Pred'] = best_i\n",
    "    results.loc[i, 'AIC'] = results_i['AIC'].min()\n",
    "    results.loc[i, 'R2'] = results_i['R2'].min()\n",
    "    results.loc[i, 'RSS'] = results_i['RSS'].min()\n",
    "\n",
    "print('Best Predictors and corresponding AIC:\\n', results,\n",
    "'\\n\\nThe best model thus contains',\n",
    "results['AIC'].argmin() + 1, ' predictors') # change this to needed metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b2cc37",
   "metadata": {},
   "source": [
    "#### Backward Stepwise Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb68578",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8ed6ab2",
   "metadata": {},
   "source": [
    "#### Anova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b065b429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 f) anova test\n",
    "x_opt = x_ii[['runtime', 'run/max', 'age', 'weight']]\n",
    "x_opt_sm = sm.add_constant(x_opt)  # Add constant\n",
    "model_opt = sm.OLS(y, x_opt_sm).fit()  # Fit the model\n",
    "\n",
    "# Anova Test:\n",
    "table = sm.stats.anova_lm(model, model_opt)\n",
    "print('\\nANOVA Test:\\n', table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfca65b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FS25_STAT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
